# Foundation Models
Foundation Models Based on Modality.

## Vision-Based Models

*Depth Anything*: HKU and Tiktok, 2024 - [Paper](https://arxiv.org/pdf/2406.09414)

*DINOv2*: Meta AI, 2024 - [Paper](https://arxiv.org/pdf/2304.07193)

*SAM (Segment Anything Model)*: Meta AI, 2023 - [Paper](https://arxiv.org/pdf/2304.02643)

*SAM 2*: Meta FAIR, 2024 - [paper](https://scontent-hou1-1.xx.fbcdn.net/v/t39.2365-6/453323338_287900751050452_6064535069828837026_n.pdf?_nc_cat=107&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=TnvI-AaGawoQ7kNvgEOJCXb&_nc_ht=scontent-hou1-1.xx&oh=00_AYAi98_rkXI2UXDI3qN4a2ZrRPn9hzdxkyhXuJX5mBYkOQ&oe=66AECA39)

*YOLO-NAS*: [code](https://github.com/Deci-AI/super-gradients/tree/master)

*ByteTrack*: ByteDance 2022 - [paper](https://github.com/Deci-AI/super-gradients/tree/master)


## Multi-Modal Vision Models

*Grounding DINO*: Microsoft Research, 2024 - [Paper](https://arxiv.org/pdf/2303.05499)

*Grounded SAM*: 2024 - [paper](https://arxiv.org/pdf/2401.14159)

*YOLO-World*: 2024 - [paper](https://arxiv.org/pdf/2401.17270)

## Vision-Language Models


*CLIP (Contrastive Languageâ€“Image Pretraining)*: OpenAI, 2021 - [Paper](https://arxiv.org/abs/2103.00020)

*EVA CLIP*: 2023 - [paper](https://arxiv.org/pdf/2303.15389)

*SigLIP*: Google, 2023 - [paper](https://arxiv.org/pdf/2303.15343)

*PaliGemma*: Google, 2024 - [paper](https://arxiv.org/pdf/2407.07726)

*Florence* (Microsoft, 2021 - [Paper](https://arxiv.org/pdf/2111.11432)

*VLMo (Vision-Language Model)*: Microsoft, 2022 - [Paper](https://arxiv.org/pdf/2111.02358)

*FLAVA (Foundational Language and Vision AI)*: Meta AI, 2022 - [Paper](https://arxiv.org/abs/2112.04482)

*MaskVLM*: Amazon, 2023 - [paper](https://arxiv.org/pdf/2208.02131)

*ALIGN (Image and Language Pre-training)*: Google Research, 2021 - [Paper](https://arxiv.org/abs/2102.05918)

*LLaVA (Large Language and Vision Assistant)*: Microsoft Research , 2023 - [Paper](https://arxiv.org/abs/2304.08485)

*LLaVA-1.5*: Microsoft research, 2024 - [paper](https://arxiv.org/pdf/2310.03744)

*LLaVA-Next*: 2024 - [Paper](https://arxiv.org/pdf/2407.07895)

*Qwen-VL*: Alibaba Group, 2023 - [Paper](https://arxiv.org/pdf/2308.12966)

*OWL ViT*: Google, 2022 - [paper](https://arxiv.org/pdf/2205.06230)

*VLPart*: Facebook research - [paper](https://arxiv.org/pdf/2305.11173)

*CogVLM*: 2023 - [paper](https://arxiv.org/pdf/2311.03079)

*GPT-4V(ision)*: 2023 - [paper](https://arxiv.org/pdf/2311.01361)

*MiniGPT4*: 2023 - [paper](https://arxiv.org/pdf/2304.10592)

*MiniGPT5*: 2024 - [paper](https://arxiv.org/pdf/2310.02239)

## Spatial-Reasoning based Models

*SpatialVLM*: Google Research and DeepMind, 2024 - [paper](https://arxiv.org/pdf/2401.12168)

*SpatislRGBT* 2024 - [paper](https://arxiv.org/pdf/2406.01584)

## Text-Based Models/ Llms

*llama 3.1*: Meta AI, 2024 - [paper](https://scontent-hou1-1.xx.fbcdn.net/v/t39.2365-6/452387774_1036916434819166_4173978747091533306_n.pdf?_nc_cat=104&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=DTS7hDTcxZoQ7kNvgF4bFEz&_nc_ht=scontent-hou1-1.xx&oh=00_AYAqN_LRjPCy1pIH8_C5ac0I3-E2LiS33XcTo-ZkX_J7qQ&oe=66AD830D)

*mistral*: 2023 - [paper](https://arxiv.org/pdf/2310.06825)

*GPT-3*: OpenAI, 2020 - [paper](https://arxiv.org/pdf/2005.14165)

*GPT-4*: OpenAI, 2023 - [paper](https://cdn.openai.com/papers/gpt-4.pdf)

*Gemini 1.5*: Google DeepMind, 2024 - [paper](https://arxiv.org/pdf/2403.05530)

*PaLM*: Google Research, 2022 - [paper](https://arxiv.org/pdf/2204.02311)

*Gopher*: DeepMind, 2022 - [paper](https://arxiv.org/pdf/2112.11446)

*BLOOM*: 2023 - [paper](https://arxiv.org/pdf/2211.05100)

*Qwen* : ALibaba Group, 2023 - [paper](https://arxiv.org/pdf/2309.16609)

*OPT: Open Pre-trained Transformer Language Models*: Meta AI, 2022 - [paper](https://arxiv.org/pdf/2205.01068)


## Audio-Based Models

*Wav2Vec*: (Facebook AI, now Meta AI, 2019 - [Paper](https://arxiv.org/pdf/1904.05862)

*Wav2Vec 2.0*: Facebook AI, now Meta AI, 2020 - [Paper](https://arxiv.org/abs/2006.11477)

*Speech2Text*: Fairseq, Facebook AI, now Meta AI, 2022 - [Paper](https://arxiv.org/pdf/2010.05171)

*AudioCLIP*: 2021 - [Paper](https://arxiv.org/abs/2106.13043)

*Whisper** OpenAI, 2022 - [Paper](https://cdn.openai.com/papers/whisper.pdf)

## Language-Vision-Audio Models

*VATT (Video-Audio-Text Transformer)*: Google Research, 2021 - [Paper](https://arxiv.org/abs/2104.11178)

*ImageBind*:  Meta AI, 2023 - [paper](https://arxiv.org/pdf/2305.05665): Images, text, audio, depth, thermal, and IMU data 

*GPT-4o* OpenAI, 2024 - [website](https://openai.com/index/hello-gpt-4o/)

## Video Generation Models

*Sora*: OpenaI, 2024 - [Technical Report](https://openai.com/index/video-generation-models-as-world-simulators/)

*VideoGPT*: 2021 - [Paper](https://arxiv.org/abs/2104.10157)

*CogVideo*: Tsinghua University, 2022 - [Paper](https://arxiv.org/abs/2205.15868)

*Make-A-Video* (Meta AI, 2022 - [Paper](https://arxiv.org/abs/2209.14792)

*Phenaki* (Google Research, 2022 - [Paper](https://arxiv.org/abs/2210.02399)

## Video Captioning/understanding

*PLLaVA*: 2024 - [Paper](https://arxiv.org/pdf/2404.16994)

*Vid2Seq*: Google Research, 2023 - [Paper](https://arxiv.org/pdf/2302.14115)

*InternVideo*: 2022 - [Paper](https://arxiv.org/pdf/2212.03191)

## Specialized Multi-Modal Models

*RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control*: DeepMind, 2023 - [paper](https://arxiv.org/pdf/2307.15818)

*Perceiver* DeepMind, 2021 - [Paper](https://arxiv.org/abs/2103.03206)


## GenAI

*DALL-E 3*: OpenAI - [website](https://openai.com/index/dall-e-3/)
